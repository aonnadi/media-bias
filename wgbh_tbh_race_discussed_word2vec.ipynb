{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "class MeanEmbeddingVectorizer:\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size  # Dimensionality of word vectors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "# Tokenize the sentences (assuming they are not tokenized already)\n",
    "tokenized_X_train = [sentence.split() for sentence in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define Word Embedding Vectorizer\n",
    "word_embedder = MeanEmbeddingVectorizer(word2vec_model.wv)\n",
    "\n",
    "# Classifier\n",
    "clf = LinearSVC()\n",
    "\n",
    "# Define pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('word_embedder', word_embedder),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "text_clf.fit(tokenized_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class MeanEmbeddingVectorizer(TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size  # Dimensionality of word vectors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv('datasets/wgbh-only-dataset_new_col.csv')\n",
    "df2 = pd.read_csv('bg_articles/tbg_results.csv')\n",
    "# Concatenating datasets row-wise (adding more entries)\n",
    "combined_df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "\n",
    "X = combined_df['text']\n",
    "y = combined_df['race_discussed']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Train Word2Vec model\n",
    "tokenized_X_train = [sentence.split() for sentence in X_train]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define Word Embedding Vectorizer\n",
    "word_embedder = MeanEmbeddingVectorizer(word2vec_model.wv)\n",
    "\n",
    "# Classifier\n",
    "clf = LinearSVC()\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('word_embedder', word_embedder),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Parameters for Grid Search\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(tokenized_X_train, y_train)\n",
    "\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(metrics.accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(TransformerMixin):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size  # Dimensionality of word vectors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv('datasets/wgbh-only-dataset_new_col.csv')\n",
    "df2 = pd.read_csv('bg_articles/tbg_results.csv')\n",
    "# Concatenating datasets row-wise (adding more entries)\n",
    "combined_df = pd.concat([df, df2], axis=0, ignore_index=True)\n",
    "\n",
    "X = combined_df['text']\n",
    "y = combined_df['race_discussed']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Define Word Embedding Vectorizer\n",
    "class WordEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word2vec_model=None, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Not used in this transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [text.split() for text in X]\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'word2vec_model': self.word2vec_model,\n",
    "            'vector_size': self.vector_size,\n",
    "            'window': self.window,\n",
    "            'min_count': self.min_count,\n",
    "            'workers': self.workers\n",
    "        }\n",
    "\n",
    "\n",
    "# Parameters for Grid Search\n",
    "param_grid = {\n",
    "    'word_embedder__word2vec__vector_size': [50, 100, 150, 200, 250, 300],\n",
    "    'clf__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('word_embedder', Pipeline([\n",
    "        ('word2vec', WordEmbeddingVectorizer(Word2Vec(vector_size=100, window=5, min_count=1, workers=4))),\n",
    "        ('mean_embed', MeanEmbeddingVectorizer(Word2Vec(vector_size=100, window=5, min_count=1, workers=4).wv))\n",
    "    ])),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
